{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy.interpolate\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess as  sm_lowess\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file containing fragment features, including count, sequence, kmer content, etc.\n",
    "\n",
    "Create a list of the kmers that don't include 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('key_real_adj_features_kmers.csv', index_col=0)\n",
    "# df = pd.read_csv('key_adj_features_kmers.csv', index_col=0)\n",
    "# df = pd.read_csv('SRR10199724_no_dupes.csv', index_col=0)\n",
    "# df = pd.read_csv('SRR10199716_sim_no_dupes.csv', index_col=0)\n",
    "# df = pd.read_csv('SRR10199716_no_dupes.csv', index_col=0)\n",
    "# df = pd.read_csv('ecori_agei_no_dupes.csv', index_col=0)\n",
    "# df = pd.read_csv('./SRR5298272.csv', index_col=0)\n",
    "df = pd.read_csv('./SRR5298272_no_dupes.csv', index_col=0)\n",
    "\n",
    "\n",
    "kmers_ls = df.columns.to_list()[15:-1]\n",
    "kmers_ls = [i for i in kmers_ls if 'N' not in i]\n",
    "print(kmers_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents the number of reads recovered after recreating fragments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['observed'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all fragments with internal cut sites. This makes the ratio comparisons much simpler, as complete digest fragments will *always* occur at a higher ratio than the longer fragments that may contain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0])\n",
    "df = df[df['internal']==0]\n",
    "print(df.shape[0])\n",
    "gen_ls = list(df['genome'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a lowess curve within the fragment distribution of each genome to find possible outliers\n",
    "\n",
    "following https://james-brennan.github.io/posts/lowess_conf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, y, xgrid):\n",
    "    '''\n",
    "    creates lowess curve for input x y arrays\n",
    "    '''\n",
    "    sample_no = round(len(y)*.05)\n",
    "    samples = np.random.choice(len(x), sample_no, replace=True)\n",
    "    y_s = y[samples]\n",
    "    x_s = x[samples]\n",
    "    y_sm = sm_lowess(y_s,x_s, frac=1./5., it=5,\n",
    "                     return_sorted = False)\n",
    "\n",
    "    # regularly sample it onto the grid\n",
    "    y_grid = scipy.interpolate.interp1d(x_s, y_sm, \n",
    "                                        fill_value='extrapolate')(xgrid)\n",
    "    return y_grid\n",
    "\n",
    "df[['low', 'high']] = 0\n",
    "\n",
    "outliers_df = pd.DataFrame()\n",
    "\n",
    "for gen in gen_ls:\n",
    "    tmp_df = df[df['genome'] == gen].copy()\n",
    "    x = np.array(tmp_df['length'].to_list())\n",
    "    y = np.array(tmp_df['observed'].to_list())\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    plt.rc(\"axes.spines\", top=False, right=False)\n",
    "    sns.set_context(\"paper\")\n",
    "\n",
    "    # use bootstrapping to get a stack of lowess models\n",
    "    xgrid = np.array([i for i in range(x.min(), x.max()+1)])\n",
    "    K = 1000\n",
    "    smooths = np.stack([smooth(x, y, xgrid) for k in range(K)]).T\n",
    "    mean = np.nanmean(smooths, axis=1)\n",
    "    std_dev = np.nanstd(smooths, axis=1)\n",
    "\n",
    "    plt.plot(xgrid, smooths, color='tomato', alpha=0.25)\n",
    "    plt.plot(x, y, 'k.', alpha=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    for i in range(0, 6):\n",
    "        low_dt = {i:j for i, j in zip(xgrid, mean-i*std_dev)}\n",
    "        high_dt = {i:j for i, j in zip(xgrid, mean+i*std_dev)}\n",
    "        tmp_df['low'] = np.where(tmp_df['observed'] < tmp_df['length'].map(low_dt), i, tmp_df['low'])\n",
    "        tmp_df['high'] = np.where(tmp_df['observed'] > tmp_df['length'].map(high_dt), i, tmp_df['high'])\n",
    "\n",
    "    outliers_df = pd.concat([outliers_df, tmp_df])\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.fill_between(xgrid, mean-1*std_dev,\n",
    "                        mean+1*std_dev, alpha=0.2,color='green')\n",
    "    plt.fill_between(xgrid, mean-2*std_dev,\n",
    "                        mean+2*std_dev, alpha=0.2,color='green')\n",
    "    plt.fill_between(xgrid, mean-3*std_dev,\n",
    "                        mean+3*std_dev, alpha=0.2,color='green')\n",
    "    plt.fill_between(xgrid, mean-4*std_dev,\n",
    "                        mean+4*std_dev, alpha=0.2,color='green')\n",
    "    plt.plot(xgrid, mean, color='tomato')\n",
    "    plt.plot(x, y, 'k.', alpha=0.5)\n",
    "    plt.title(f'Stdevs from bootstrapped Lowess model\\n{gen}')\n",
    "    plt.show()\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the ratio of taxa-to-taxa fragment counts for each fragment length\n",
    "\n",
    "Calculate all relative abundance comparisons by capturing the inter-taxa ratios for each fragment length. The average of these ratios will be used to determine the overall relative abundance of the taxa, because the ratios should hold regardless of the fragment size being taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ground truth list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_ls = []\n",
    "for gen in gen_ls:\n",
    "    e = df[df['genome']==gen]['rel_abund'].unique()[0]\n",
    "    e_ls.append(float(e))\n",
    "\n",
    "# print(e_ls)\n",
    "print(len(e_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ratios(tmp_df, gen_ls):\n",
    "    for gen in gen_ls:\n",
    "        avg = tmp_df[tmp_df['genome']==gen]['observed'].mean()\n",
    "        tmp_df[gen] = tmp_df['observed'] / avg\n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def scale_ratios(np_arr):\n",
    "    rel_base = np_arr[0,0]\n",
    "    for idx, i in enumerate(np_arr[0,:]):\n",
    "        col_scale = rel_base/i\n",
    "        np_arr[:,idx] = np_arr[:,idx]*col_scale\n",
    "    return np_arr\n",
    "\n",
    "\n",
    "def average_over_columns(np_arr):\n",
    "    avg_ls = np.nanmean(np_arr, axis=1).tolist()\n",
    "    return avg_ls\n",
    "\n",
    "\n",
    "def return_rel_abund(o_ls):\n",
    "    rel_ls = []\n",
    "    for i in o_ls:\n",
    "        rel_ls.append(i/sum(o_ls))\n",
    "    return rel_ls\n",
    "\n",
    "\n",
    "for i in range(max(outliers_df['low'].max(), outliers_df['high'].max())+1, 0, -1):\n",
    "    print(f'using fragments within {i} standard deviations')\n",
    "    test_df = outliers_df[(outliers_df['low'] < i )  & (outliers_df['high'] < i)].copy()\n",
    "    try:\n",
    "        test_df.drop(gen_ls, inplace=True, axis=1)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    test_df = test_df.reindex(columns = test_df.columns.tolist() + gen_ls)\n",
    "    print(test_df.shape[0])\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for j in range(0, test_df['length'].max()+1):\n",
    "        tmp_df = test_df[test_df['length']==j].copy()\n",
    "        if tmp_df.shape[0] > 0:\n",
    "            tmp_df = process_ratios(tmp_df, gen_ls)\n",
    "            final_df = pd.concat([final_df, tmp_df])\n",
    "\n",
    "    ratios_df = pd.DataFrame(0, index=gen_ls, columns=gen_ls)\n",
    "\n",
    "    for gena in gen_ls:\n",
    "        for genb in gen_ls:\n",
    "            ratios_df.loc[gena, genb] = final_df[final_df['genome']==gena][genb].mean()\n",
    "\n",
    "    np_ratios = np.array(ratios_df)\n",
    "\n",
    "    scaled_arr = scale_ratios(np_ratios)\n",
    "    o_ls = average_over_columns(scaled_arr)\n",
    "    o_ls = return_rel_abund(o_ls)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(e_ls, o_ls)\n",
    "    plt.plot([0,.12],[0,.12])\n",
    "    for i, gen_name in enumerate(gen_ls):\n",
    "        plt.annotate(gen_name, (e_ls[i]+.003, o_ls[i]), fontsize=11)\n",
    "    plt.show()\n",
    "    print(f'{pearsonr(e_ls,o_ls)}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean observed number of reads at each length, n. Then, create a n x n matrix of every mean ratio across all lengths where present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmers_sums = outliers_df[kmers_ls].sum().to_list()\n",
    "kmers_bunds_start = [i/sum(kmers_sums) for i in kmers_sums]\n",
    "\n",
    "transp_ls = [i for i in range(max(outliers_df['low'].max(), outliers_df['high'].max()), 0, -1)]\n",
    "transp_ls = [i/max(outliers_df['low'].max(), outliers_df['high'].max()) for i in reversed(transp_ls)]\n",
    "\n",
    "plt.figure(figsize=(20,5))  \n",
    "plt.cool()\n",
    "     \n",
    "for idx, i in enumerate(range(max(outliers_df['low'].max(), outliers_df['high'].max()), 0, -1)):\n",
    "    # print(f'using fragments within {i} standard deviations')\n",
    "    kmer_df = outliers_df[(outliers_df['high'] < i)].copy()\n",
    "    kmers_sums = kmer_df[kmers_ls].sum().to_list()\n",
    "    kmers_bunds = [i/sum(kmers_sums) for i in kmers_sums]\n",
    "    kmers_bunds = [i-j for i, j in zip(kmers_bunds, kmers_bunds_start)]\n",
    "    plt.xticks(rotation=90)\n",
    "    points=[idx for idx, i in enumerate(kmers_ls)]\n",
    "    plt.axhline(y=0, color = 'black')\n",
    "    for i in points:\n",
    "        plt.axvline(x=i, color = 'whitesmoke', zorder=0)\n",
    "    plt.scatter(kmers_ls, kmers_bunds, s=100, c=kmers_bunds, zorder=1, alpha=transp_ls[idx])\n",
    "print('removing the overabundant fragments...')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,5))       \n",
    "\n",
    "for idx, i in enumerate(range(max(outliers_df['low'].max(), outliers_df['high'].max()), 0, -1)):\n",
    "    # print(f'using fragments within {i} standard deviations')\n",
    "    kmer_df = outliers_df[(outliers_df['low'] < i)].copy()\n",
    "    kmers_sums = kmer_df[kmers_ls].sum().to_list()\n",
    "    kmers_bunds = [i/sum(kmers_sums) for i in kmers_sums]\n",
    "    kmers_bunds = [i-j for i, j in zip(kmers_bunds, kmers_bunds_start)]\n",
    "    plt.xticks(rotation=90)\n",
    "    points=[idx for idx, i in enumerate(kmers_ls)]\n",
    "    plt.axhline(y=0, color = 'black')\n",
    "    for i in points:\n",
    "        plt.axvline(x=i, color = 'whitesmoke', zorder=0)\n",
    "    plt.scatter(kmers_ls, kmers_bunds, s=100, c=kmers_bunds, zorder=1, alpha=transp_ls[idx])\n",
    "print('removing the underabundant fragments...')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmer_df = adj_final_df.copy()\n",
    "\n",
    "# mlr_df = kmer_df[kmers_ls+['length','observed']]\n",
    "\n",
    "# train, test = train_test_split(mlr_df, test_size=0.3)\n",
    "\n",
    "# y_train = np.array(train['observed'])\n",
    "# x_train = np.array(train.drop(['observed'], axis=1))\n",
    "# y_test = np.array(test['observed'])\n",
    "# x_test = np.array(test.drop(['observed'], axis=1))\n",
    "\n",
    "# model = LinearRegression().fit(x_train, y_train)\n",
    "# preds = [model.predict(np.array([i]))[0] for i in x_test]\n",
    "\n",
    "# plt.figure(figsize=[10,10])\n",
    "# plt.scatter(preds,list(y_test),alpha=0.05)\n",
    "# plt.xlabel('predicted')\n",
    "# plt.ylabel('observed')\n",
    "# plt.plot([i for i in np.linspace(0,200,10)],[i for i in np.linspace(0,200,10)],c='pink')\n",
    "# plt.xlim([0,200])\n",
    "# plt.ylim([0,200])\n",
    "# plt.show()\n",
    "# print(pearsonr(preds,list(y_test)))\n",
    "\n",
    "# reg = MLPRegressor(hidden_layer_sizes=(x_train.shape[1], x_train.shape[1]), solver='adam', activation=\"relu\", random_state=1, max_iter=1000).fit(x_train, y_train)\n",
    "# y_pred=reg.predict(x_test)\n",
    "# print(r2_score(y_pred, y_test))\n",
    "\n",
    "# plt.figure(figsize=[10,10])\n",
    "# plt.scatter(y_pred,list(y_test),alpha=0.05)\n",
    "# plt.xlabel('predicted')\n",
    "# plt.ylabel('observed')\n",
    "# plt.plot([i for i in np.linspace(0,200,10)],[i for i in np.linspace(0,200,10)],c='pink')\n",
    "# plt.xlim([0,200])\n",
    "# plt.ylim([0,200])\n",
    "# plt.show()\n",
    "# print(pearsonr(list(y_pred),list(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
