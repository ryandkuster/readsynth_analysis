{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skmisc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ac5fef66e650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mskmisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skmisc'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from skmisc.loess import loess\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file containing fragment features, including count, sequence, kmer content, etc.\n",
    "\n",
    "Create a list of the kmers that don't include 'N'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('key_real_adj_features_kmers.csv', index_col=0)\n",
    "df = pd.read_csv('key_adj_features_kmers.csv', index_col=0)\n",
    "\n",
    "kmers_ls = df.columns.to_list()[17:]\n",
    "kmers_ls = [i for i in kmers_ls if 'N' not in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all fragments with internal cut sites. This makes the ratio comparisons much simpler, as complete digest fragments will *always* occur at a higher ratio than the longer fragments that may contain them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['internal']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove fragments that map multiply within or across genomes\n",
    "\n",
    "Using a given threshold, remove fragments of identical length that map to multiple genomes and differ in hamming distance at only 25% or less over all positions. This prevents multiply-mapped reads from being over counted across genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hamming_distance(a, b):\n",
    "    hamm = 0\n",
    "    for i, j in zip(a, b):\n",
    "        if i != j:\n",
    "            hamm += 1\n",
    "    return hamm/len(a)\n",
    "\n",
    "threshold = 0.25 #rate of hamming distance\n",
    "df_dupes = pd.DataFrame()\n",
    "\n",
    "for i in range(1, df['length'].max()+1):\n",
    "    tmp_df = df[df['length']==i].copy()\n",
    "    seq_ls = tmp_df['seq'].to_list()\n",
    "    gen_ls = tmp_df['genome'].to_list()\n",
    "\n",
    "    hamm_ls = []\n",
    "    for idxa, (seqa, gena) in enumerate(zip(seq_ls, gen_ls)):\n",
    "        close_ones = 0\n",
    "        for idxb, (seqb, genb) in enumerate(zip(seq_ls, gen_ls)):\n",
    "            if idxa == idxb:\n",
    "                pass\n",
    "            else:\n",
    "                hamm = get_hamming_distance(seqa, seqb)\n",
    "                if hamm <= threshold:\n",
    "                    close_ones += 1\n",
    "        hamm_ls.append(close_ones)\n",
    "    tmp_df.loc[:,'neighbors'] = hamm_ls\n",
    "    df_dupes = pd.concat([df_dupes, tmp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_dupes[df_dupes['neighbors'] == 0]\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['difference'] = df['observed'] - df['counts']\n",
    "# df = df[(df['difference'] > -50)&(df['difference'] < 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['difference'] < -30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean observed number of reads at each length, n. Then, create a n x n matrix of every mean ratio across all lengths where present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gen_ls = list(df['genome'].unique())\n",
    "\n",
    "\n",
    "def get_means_at_len(df, gen_df):\n",
    "    means_ls = []\n",
    "    for i in range(1, df['length'].max()+1):\n",
    "        len_ls_i = gen_df[gen_df['length']==i]['observed'].to_list()\n",
    "        if len_ls_i:\n",
    "            mean_i = sum(len_ls_i)/len(len_ls_i)\n",
    "            means_ls.append(mean_i)\n",
    "        else:\n",
    "            means_ls.append(0)\n",
    "    return means_ls\n",
    "\n",
    "\n",
    "def get_means_ratios(df, means_ls):\n",
    "    len_ratios = np.empty((df['length'].max(), df['length'].max()))\n",
    "    len_ratios[:] = np.NaN\n",
    "    for idxi, i in enumerate(means_ls):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        for idxj, j in enumerate(means_ls):\n",
    "            if j != 0:\n",
    "                len_ratios[idxi, idxj] = i / j\n",
    "    return len_ratios\n",
    "\n",
    "\n",
    "for idx, gen in enumerate(gen_ls):\n",
    "    gen_df = df[df['genome']==gen].copy()\n",
    "    means_ls = get_means_at_len(df, gen_df)\n",
    "    len_ratios = get_means_ratios(df, means_ls)\n",
    "    if idx > 0:\n",
    "        comb_arr = np.append(comb_arr, np.array([len_ratios]), axis=0)\n",
    "    else:\n",
    "        comb_arr = np.array([len_ratios])\n",
    "\n",
    "comb_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plotting outliers based on read ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "from skmisc.loess import loess\n",
    "\n",
    "z = np.nanmean(comb_arr,axis=0)\n",
    "width = comb_arr.shape[2]\n",
    "x = np.linspace(1, 506, 506)\n",
    "y = np.linspace(1, 506, 506)\n",
    "x, y = np.meshgrid(x, y)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,15), subplot_kw=dict(projection='3d'))\n",
    "surf = ax.plot_surface(x, y, z, rstride=5, cstride=5, linewidth=0, antialiased=True,alpha=0.75)\n",
    "# ax.view_init(30, -150)\n",
    "plt.show()\n",
    "\n",
    "row_view = np.nanmean(z,axis=0).tolist()\n",
    "x_ls = [i for i in range(len(row_view))]\n",
    "plt.scatter(x_ls, row_view, alpha=0.05)\n",
    "# lowess = sm.nonparametric.lowess\n",
    "# modelr = lowess(np.array(row_view), np.array(x_ls), frac=0.1)\n",
    "# plt.scatter(modelr[:,0], modelr[:,1], color='red', s=1)\n",
    "x = np.array(x_ls)\n",
    "y = np.array(row_view)\n",
    "l = loess(x,y)\n",
    "l.fit()\n",
    "pred = l.predict(x, stderror=True)\n",
    "conf = pred.confidence()\n",
    "\n",
    "lowess = pred.values\n",
    "ll = conf.lower\n",
    "ul = conf.upper\n",
    "\n",
    "plt.plot(x, y, '+')\n",
    "plt.plot(x, lowess)\n",
    "plt.fill_between(x,ll,ul,alpha=.33)\n",
    "plt.show()\n",
    "\n",
    "col_view = np.nanmean(z,axis=1).tolist()\n",
    "plt.scatter(x_ls, col_view, alpha=0.15)\n",
    "lowess = sm.nonparametric.lowess\n",
    "modelc = lowess(np.array(col_view), np.array(x_ls), frac=0.2)\n",
    "plt.scatter(modelc[:,0], modelc[:,1], color='red', s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, i in enumerate(comb_arr):\n",
    "    z = comb_arr[idx,:,:]\n",
    "    x = np.linspace(1, 506, 506)\n",
    "    y = np.linspace(1, 506, 506)\n",
    "    x, y = np.meshgrid(x, y)\n",
    "    fig, ax = plt.subplots(figsize=(20,20), subplot_kw=dict(projection='3d'))\n",
    "    surf = ax.plot_surface(x, y, z, rstride=5, cstride=5, linewidth=0, antialiased=True, alpha=0.75)\n",
    "#     ax.view_init(20, -160)\n",
    "    plt.title(gen_ls[idx])\n",
    "    plt.show()\n",
    "    \n",
    "    row_view = np.nanmean(z,axis=0).tolist()\n",
    "    plt.scatter([i for i in range(len(row_view))], row_view)\n",
    "    plt.scatter(modelr[:,0], modelr[:,1], color='red', s=1)\n",
    "    plt.show()\n",
    "    \n",
    "    col_view = np.nanmean(z,axis=1).tolist()\n",
    "    plt.scatter([i for i in range(len(col_view))], col_view)\n",
    "    plt.scatter(modelc[:,0], modelc[:,1], color='red', s=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the ratio of taxa-to-taxa fragment counts for each fragment length\n",
    "\n",
    "Calculate all relative abundance comparisons by capturing the inter-taxa ratios for each fragment length. The average of these ratios will be used to determine the overall relative abundance of the taxa, because the ratios should hold regardless of the fragment size being taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_ls = list(df['genome'].unique())\n",
    "test_df = df.reindex(columns = df.columns.tolist() + gen_ls)\n",
    "\n",
    "def process_ratios(tmp_df, gen_ls):\n",
    "    for gen in gen_ls:\n",
    "#         avg = tmp_df[tmp_df['genome']==gen]['observed'].mean()\n",
    "#         tmp_df[gen] = tmp_df['observed'] / avg\n",
    "        med = tmp_df[tmp_df['genome']==gen]['observed'].median()\n",
    "        tmp_df[gen] = tmp_df['observed'] / med\n",
    "        frags = tmp_df[tmp_df['genome']==gen].shape[0]\n",
    "        if frags > 0:\n",
    "            tmp_df['frags_per_len'] = frags\n",
    "    return tmp_df\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, test_df['length'].max()+1):\n",
    "    tmp_df = test_df[test_df['length']==i].copy()\n",
    "    tmp_df = tmp_df.reindex(columns = tmp_df.columns.tolist() + ['frags_per_len'])\n",
    "    if tmp_df.shape[0] > 0:\n",
    "        tmp_df = process_ratios(tmp_df, gen_ls)\n",
    "        final_df = pd.concat([final_df, tmp_df])\n",
    "\n",
    "ratios_df = pd.DataFrame(0, index=gen_ls, columns=gen_ls)\n",
    "final_df['prevalence'] = 0\n",
    "\n",
    "for gena in gen_ls:\n",
    "    for genb in gen_ls:\n",
    "        ratios_df.loc[gena, genb] = final_df[final_df['genome']==gena][genb].mean()\n",
    "        if gena == genb:\n",
    "            final_df.loc[final_df['genome']==gena, 'prevalence'] = final_df[gena]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ratios = np.array(ratios_df)\n",
    "\n",
    "for i, gen in zip(range(0,20), gen_ls):\n",
    "    abundance_col = np_ratios[:, i]\n",
    "\n",
    "    o_ls = []\n",
    "    e_ls = []\n",
    "\n",
    "    for i, j in zip(gen_ls, abundance_col):\n",
    "        a = final_df[final_df['genome']==i]['rel_abund'].unique()[0]\n",
    "        b = j/abundance_col.sum()\n",
    "        a = '{0:.4f}'.format(a)\n",
    "        b = '{0:.4f}'.format(b)\n",
    "        o_ls.append(float(b))\n",
    "        e_ls.append(float(a))\n",
    "    rows = final_df[final_df['genome'] == gen].shape[0]\n",
    "    frag_weight = final_df[final_df['genome'] == gen]['frags_per_len'].mean()*rows\n",
    "    connects = (final_df[gen].count()-rows)/rows\n",
    "    print(f'{pearsonr(e_ls,o_ls)}  {frag_weight}  {rows}  {connects}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide every fragment estimate of relative ratios, per-genome, by the average estimated ratio for that taxon (found in the ratios matrix row for that genome). If an individual fragment's ratio is consistently higher or lower than the average ratios calculated vs each of the other taxa, then it is anticipated that that fragment may be overly present in the library due to bias (represented as 'adj_prev' greater than or less than 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adj_final_df = pd.DataFrame()\n",
    "\n",
    "for gena in gen_ls:\n",
    "    tmp_df = final_df[final_df['genome'] == gena]\n",
    "    new_df = tmp_df[gen_ls] / ratios_df.loc[gena]\n",
    "    means_ls = new_df.mean(axis=1).tolist()\n",
    "    tmp_df['adj_prev'] = means_ls\n",
    "    adj_final_df = pd.concat([adj_final_df, tmp_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing for outliers\n",
    "\n",
    "Similar to the logic used above to produce the 'adj_prev' feature, here we create two columns 'over' and 'under' to capture the number of times an individual fragment falls outside the distribution of the ratios observed. For example, taxon A may fall short of the expected ratio in comparison to taxon B, but this could be due to an inflated value for B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj_final_df[['over', 'under']] = 0\n",
    "\n",
    "for gena in gen_ls:\n",
    "    for genb in gen_ls:\n",
    "        ratios_ls = adj_final_df[adj_final_df['genome']==gena][genb].to_list()\n",
    "        ratios_ls = [i for i in ratios_ls if i > 0]\n",
    "        ratios_arr = np.array(ratios_ls)\n",
    "        min_ratio = np.quantile(ratios_arr,0.25)\n",
    "        max_ratio = np.quantile(ratios_arr,.75)\n",
    "        adj_final_df.loc[(adj_final_df['genome']==gena) & (adj_final_df[genb]>max_ratio), 'over'] += 1\n",
    "        adj_final_df.loc[(adj_final_df['genome']==gena) & (adj_final_df[genb]<min_ratio), 'under'] += 1\n",
    "\n",
    "#         print(f'{gena} vs {genb}')\n",
    "#         plt.boxplot(ratios_ls)\n",
    "#         plt.axhline(y=min_ratio)\n",
    "#         plt.axhline(y=max_ratio)\n",
    "#         plt.show()\n",
    "#         input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# kmer_df = adj_final_df.copy()\n",
    "# kmers_sums = adj_final_df[kmers_ls].sum().to_list()\n",
    "# kmers_bunds_start = [i/sum(kmers_sums) for i in kmers_sums]\n",
    "        \n",
    "# for i in range(0, 20):\n",
    "#     kmer_df = adj_final_df[(adj_final_df['over'] >= i )]\n",
    "#     print(f'keep fragments where \\'over\\' is {i} or greater ({kmer_df.shape[0]} fragments)')\n",
    "#     kmers_sums = kmer_df[kmers_ls].sum().to_list()\n",
    "#     kmers_bunds = [i/sum(kmers_sums) for i in kmers_sums]\n",
    "#     kmers_bunds = [i-j for i, j in zip(kmers_bunds, kmers_bunds_start)]\n",
    "#     plt.figure(figsize=(20,5))\n",
    "#     plt.xticks(rotation=90)\n",
    "#     points=[idx for idx, i in enumerate(kmers_ls)]\n",
    "#     plt.axhline(y=0, color = 'black')\n",
    "#     for i in points:\n",
    "#         plt.axvline(x=i, color = 'whitesmoke', zorder=0)\n",
    "#     plt.scatter(kmers_ls, kmers_bunds, s=100, c=kmers_bunds, zorder=1)\n",
    "#     plt.cool()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def process_ratios(tmp_df, gen_ls):\n",
    "    for gen in gen_ls:\n",
    "        avg = tmp_df[tmp_df['genome']==gen]['observed'].mean()\n",
    "        tmp_df[gen] = tmp_df['observed'] / avg\n",
    "    return tmp_df\n",
    "\n",
    "\n",
    "def scale_ratios(np_arr):\n",
    "    rel_base = np_arr[0,0]\n",
    "    for idx, i in enumerate(np_arr[0,:]):\n",
    "        col_scale = rel_base/i\n",
    "        np_arr[:,idx] = np_arr[:,idx]*col_scale\n",
    "    return np_arr\n",
    "\n",
    "\n",
    "def average_over_columns(np_arr):\n",
    "    avg_ls = np.nanmean(np_arr, axis=1).tolist()\n",
    "    return avg_ls\n",
    "\n",
    "\n",
    "def return_rel_abund(o_ls):\n",
    "    rel_ls = []\n",
    "    for i in o_ls:\n",
    "        rel_ls.append(i/sum(o_ls))\n",
    "    return rel_ls\n",
    "\n",
    "\n",
    "e_ls = []\n",
    "for genb, ratio in zip(gen_ls, abundance_col):\n",
    "    e = adj_final_df[adj_final_df['genome']==genb]['rel_abund'].unique()[0]\n",
    "    e_ls.append(float(e))\n",
    "\n",
    "    \n",
    "# for i in range(18,-1,-1):\n",
    "#     print(f'using fragments with over and under less than {i}')\n",
    "#     kmer_df = adj_final_df[(adj_final_df['over'] <= i )  & (adj_final_df['under'] <= i )].copy()\n",
    "\n",
    "for i in np.linspace(3,1,10):\n",
    "    print(f'using fragments with adj_prev between {1/i} and {i}')\n",
    "    kmer_df = adj_final_df[(adj_final_df['prevalence'] <= i )  & (adj_final_df['prevalence'] >= 1/i )].copy()\n",
    "\n",
    "    kmer_df.drop(gen_ls,inplace=True,axis=1)\n",
    "    test_df = kmer_df.reindex(columns = kmer_df.columns.tolist() + gen_ls)\n",
    "    print(test_df.shape[0])\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for j in range(0, test_df['length'].max()+1):\n",
    "        tmp_df = test_df[test_df['length']==j].copy()\n",
    "        if tmp_df.shape[0] > 0:\n",
    "            tmp_df = process_ratios(tmp_df, gen_ls)\n",
    "            final_df = pd.concat([final_df, tmp_df])\n",
    "\n",
    "    ratios_df = pd.DataFrame(0, index=gen_ls, columns=gen_ls)\n",
    "    final_df['prevalence'] = 0\n",
    "\n",
    "    for gena in gen_ls:\n",
    "        for genb in gen_ls:\n",
    "            ratios_df.loc[gena, genb] = final_df[final_df['genome']==gena][genb].mean()\n",
    "            if gena == genb:\n",
    "                final_df.loc[final_df['genome']==gena, 'prevalence'] = final_df[gena]\n",
    "\n",
    "    np_ratios = np.array(ratios_df)\n",
    "    scaled_arr = scale_ratios(np_ratios)\n",
    "    o_ls = average_over_columns(scaled_arr)\n",
    "    o_ls = return_rel_abund(o_ls)\n",
    "    plt.scatter(e_ls, o_ls)\n",
    "    plt.plot([0,.12],[0,.12])\n",
    "    plt.show()\n",
    "    print(f'{pearsonr(e_ls,o_ls)}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(o_ls, e_ls)\n",
    "plt.plot([0,.13],[0,.13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_df = adj_final_df.copy()\n",
    "\n",
    "mlr_df = kmer_df[kmers_ls+['length','observed']]\n",
    "\n",
    "train, test = train_test_split(mlr_df, test_size=0.3)\n",
    "\n",
    "y_train = np.array(train['observed'])\n",
    "x_train = np.array(train.drop(['observed'], axis=1))\n",
    "y_test = np.array(test['observed'])\n",
    "x_test = np.array(test.drop(['observed'], axis=1))\n",
    "\n",
    "model = LinearRegression().fit(x_train, y_train)\n",
    "preds = [model.predict(np.array([i]))[0] for i in x_test]\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.scatter(preds,list(y_test),alpha=0.05)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('observed')\n",
    "plt.plot([i for i in np.linspace(0,200,10)],[i for i in np.linspace(0,200,10)],c='pink')\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([0,200])\n",
    "plt.show()\n",
    "print(pearsonr(preds,list(y_test)))\n",
    "\n",
    "reg = MLPRegressor(hidden_layer_sizes=(x_train.shape[1], x_train.shape[1]), solver='adam', activation=\"relu\", random_state=1, max_iter=1000).fit(x_train, y_train)\n",
    "y_pred=reg.predict(x_test)\n",
    "print(r2_score(y_pred, y_test))\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.scatter(y_pred,list(y_test),alpha=0.05)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('observed')\n",
    "plt.plot([i for i in np.linspace(0,200,10)],[i for i in np.linspace(0,200,10)],c='pink')\n",
    "plt.xlim([0,200])\n",
    "plt.ylim([0,200])\n",
    "plt.show()\n",
    "print(pearsonr(list(y_pred),list(y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
